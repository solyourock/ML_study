{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chap_16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvE9kjH9vu5y",
        "outputId": "5061a4de-12e6-4392-c6e3-5b6a8ad603ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 17 11:59:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_saA1Zg5ya5",
        "outputId": "9d56e48d-d30f-4b8c-aa0d-b2d133d5905c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 26.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 30.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 31.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 파이썬 ≥3.5 필수\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# 사이킷런 ≥0.20 필수\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version은 코랩에서만 동작합니다.\n",
        "    %tensorflow_version 2.x\n",
        "    %pip install -q -U tensorflow-addons\n",
        "    %pip install -q -U transformers\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# 텐서플로 ≥2.0 필수\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"감지된 GPU가 없습니다. GPU가 없으면 LSTM과 CNN이 매우 느릴 수 있습니다.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"런타임 > 런타임 유형 변경 메뉴를 선택하고 하드웨어 가속기로 GPU를 고르세요.\")\n",
        "\n",
        "# 공통 모듈 임포트\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 깔끔한 그래프 출력을 위해\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# 그림을 저장할 위치\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"그림 저장\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Char-RNN"
      ],
      "metadata": {
        "id": "A0Mlt97s581R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7TFyOqg53mL",
        "outputId": "f0981c15-65a4-4a1b-ab04-cf56f3c6d209"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:148])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cVmOCc-6AZA",
        "outputId": "d7f75a5a-3118-4a12-95f8-0ab23eab4409"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XY1hL_hn6DK8",
        "outputId": "c5f2c89b-5bc6-4728-8718-48835ffe48b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩\n",
        "# char_level : 단어 수준 인코딩 대신 글자 수준 인코딩, 기본적으로 소문자로 바꿔줌 (lower=False 소문자로 바꾸지 않을 경우)"
      ],
      "metadata": {
        "id": "EfujCyg06Fw3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "metadata": {
        "id": "X9OSvwMR6KGI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0sbvZ5t6MKW",
        "outputId": "8f90641f-8f8d-4164-dda2-14c8228825df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jm6BhI-6NTM",
        "outputId": "45249e7a-e6aa-4c62-b71e-1e634e54ef01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_id = len(tokenizer.word_index) # 고유한 문자 개수\n",
        "dataset_size = tokenizer.document_count # 전체 문자 개수"
      ],
      "metadata": {
        "id": "itmEGLCA6Phs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 텍스트를 인코딩해서 각 글자를 ID로 (1에서 39까지가 아닌 0에서 38까지로 얻기 위해 1을 뺸다)\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "# 테스트 트레인 split\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "metadata": {
        "id": "x_yPDmF96Rr0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# window() 메서드를 사용해서 긴 시퀸스를 작은 많은 텍스트 윈도로 변환\n",
        "# 짧은 부분 문자열만큼 역전파를 위해 펼쳐 짐 (TBPTT truncated backpropagation through time)\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # 타깃 = 한 글자 앞선 입력\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "xvuH7VkB6VR3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 중첩 데이터셋을 플랫 데이터셋으로\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "metadata": {
        "id": "apxqhnqV6a2U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "9mHkD77f6eLD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "metadata": {
        "id": "HjH27VXm6gHI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
      ],
      "metadata": {
        "id": "13V4qnkD6iZX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "cVZO_pJw6kWI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgj8xIdv6lO2",
        "outputId": "47559675-0f13-4c7e-b911-a69241a9af6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 유닛 128개를 가진 GRU 층 2개, 입력에 20% 드롭아웃\n",
        "# TimeDistributed 클래스를 적용한 Dense층\n",
        "# 타임 스텝 출력 확률의 합은 1이어야 하기 때문에 Dense층의 출력 소프트맥스 함수\n",
        "# sparse_categorical_crossentropy 손실과 adam 옵티마이저\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                     #dropout=0.2, recurrent_dropout=0.2),\n",
        "                     dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     #dropout=0.2, recurrent_dropout=0.2),\n",
        "                     dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gMg7YeN6mLn",
        "outputId": "a2460278-c22d-4656-ed24-4713ff12aba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "31368/31368 [==============================] - 1726s 54ms/step - loss: 1.6214\n",
            "Epoch 2/10\n",
            "31368/31368 [==============================] - 1634s 52ms/step - loss: 1.5325\n",
            "Epoch 3/10\n",
            "  543/31368 [..............................] - ETA: 26:50 - loss: 1.6082"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델로 텍스트 생성\n",
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)"
      ],
      "metadata": {
        "id": "La-nEFw26pV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "#Y_pred = model.predict_classes(X_new)\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 첫번째 문장의 마지막 글자"
      ],
      "metadata": {
        "id": "2Yb3ouYH6xpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가짜 셰익스피어 텍스트 생성\n",
        "# char-rnn 모델 같은 단어 반복되는 경우 많음 tf.random.categorical() 함수 사용해 모델이 추정한 확률을 기반으로\n",
        "# 다음 글자를 무작위로 선택할 수 있음 tf.random.categorical() 함수는 클래스의 로그 확률(로짓)을 전달하면 랜덤하게 클래스 인덱스 샘플링\n",
        "# 생성된 텍스트의 다양성을 더 많이 제어하려면 온도라고 불리는 숫자로 로짓을 나눔 0에 가까울수록 높은 확률을 가진 글자 선택\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ],
      "metadata": {
        "id": "uhDoydhm613G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "metadata": {
        "id": "5GWxSmUJ663X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "next_char('how are yo', temperature=1)"
      ],
      "metadata": {
        "id": "m07Ip9g-Pxd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(text, n_char=50, temperature=1):\n",
        "  for _ in range(n_char):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "7IBjm4URQefx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "print(complete_text(\"t\", temperature=0.2))"
      ],
      "metadata": {
        "id": "F4gvsPmpQwls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", temperature=1))"
      ],
      "metadata": {
        "id": "JXh7ts93Q2vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", temperature=2))"
      ],
      "metadata": {
        "id": "dWNv8WGPRGyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 좀 더 좋은 텍스트를 생성하려면 GRU층과 층의 뉴런수를 더 늘리거나 더 오래 훈련하거나 규제 (recurrent_dropout=0.3으로 지정)"
      ],
      "metadata": {
        "id": "dZZrpXx3RHoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상태가 없는 RNN : 훈련 반복마다 모델의 은닉 상태를 0으로 초기화\n",
        "# 상태가 있는 RNN : 훈련 배치를 처리한 후 마지막 상태를 다음 훈련 배치의 초기 상태로 사용\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "for encoded_part in encoded_parts:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "    datasets.append(dataset)\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "ViHsBwS7RcGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                                                  dropout=0.2,\n",
        "                                                  batch_input_shape=[batch_size, None, max_id]),\n",
        "                                 keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                                                  dropout=0.2),\n",
        "                                 keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])"
      ],
      "metadata": {
        "id": "toeTQgDaS_WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begain(self, epoch, logs):\n",
        "    self.model.reset_states()"
      ],
      "metadata": {
        "id": "NtsqdRHlUP9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=50,\n",
        "          callbacks=[ResetStatesCallback()])"
      ],
      "metadata": {
        "id": "_VRi_xbBUqvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델에 다른 크기의 배치를 사용하려면 상태가 없는 복사본을 만들어야함\n",
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])"
      ],
      "metadata": {
        "id": "L1HPqik1U8tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치를 복사하려면 먼저 모델을 빌드\n",
        "stateless_model.build(tf.TensorShape([None, None, max_id]))"
      ],
      "metadata": {
        "id": "X623ddyEVgYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stateless_model.set_weights(model.get_weights())\n",
        "model = stateless_model"
      ],
      "metadata": {
        "id": "EpGdVe6QVlBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text(\"t\"))"
      ],
      "metadata": {
        "id": "QuAnixOsVmWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 감성 분석"
      ],
      "metadata": {
        "id": "ue5XDhq2VqgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
      ],
      "metadata": {
        "id": "xXxlyo-hVoI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미 정수 인코딩 처리되어 있음\n",
        "X_train[0][:10]"
      ],
      "metadata": {
        "id": "kCOTivOhV4rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "id_to_word = {id_ + 3 : word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])  "
      ],
      "metadata": {
        "id": "Qj0jXBFVV6fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_size = info.splits['train'].num_examples"
      ],
      "metadata": {
        "id": "yt4vFtcgWj4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "  X_batch = tf.strings.substr(X_batch, 0, 300) # 각 리뷰의 첫 300자만\n",
        "  X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \") # <br />태그를 공백으로 변환\n",
        "  X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "  X_batch = tf.strings.split(X_batch)\n",
        "  return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch # <pad>로 모든 리뷰 패딩"
      ],
      "metadata": {
        "id": "lDafPhDFXjnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "mcz681KjlN8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(X_batch, y_batch)"
      ],
      "metadata": {
        "id": "MvG9v1oJX8SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘 사전 구축\n",
        "from collections import Counter\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
        "  for review in X_batch:\n",
        "    vocabulary.update(list(review.numpy()))"
      ],
      "metadata": {
        "id": "VZql-udrZsPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.most_common()[:3]"
      ],
      "metadata": {
        "id": "0BDcInOVaHlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 많이 등장하는 단어 10,000개\n",
        "vocab_size = 10000\n",
        "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"
      ],
      "metadata": {
        "id": "TxWH4XVXaKYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This movie was faaaaantastic\".split():\n",
        "  print(word_to_id.get(word) or vocab_size)"
      ],
      "metadata": {
        "id": "IQazfjPdhlpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1,000개의 OOV out-of-vocabulary 버킷을 사용하는 룩업 테이블\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ],
      "metadata": {
        "id": "B9_xv5pjh6QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this, movie, was는 룩업 테이블에 존재, faaaaantastic는 없어서 10,000보다 크거나 같은 ID를 가진 oov 버킷 중 하나에 매핑\n",
        "table.lookup(tf.constant([b\"This movie was faaaaantastic\".split()]))"
      ],
      "metadata": {
        "id": "Jrw410IajZQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "  return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)"
      ],
      "metadata": {
        "id": "xb42_z4JjmUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_batch, y_batch in train_set.take(1):\n",
        "  print(X_batch)\n",
        "  print(y_batch)"
      ],
      "metadata": {
        "id": "kfKCE_EekKRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                                                        input_shape=[None]),\n",
        "                                 keras.layers.GRU(128, return_sequences=True),\n",
        "                                 keras.layers.GRU(128),\n",
        "                                 keras.layers.Dense(1, activation=\"sigmoid\")                          \n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "id": "W8A8FEo9lsX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마스킹\n",
        "# Embedding층을 만들때 mask_zero=True 추가하면 패딩 토큰을 무시\n",
        "K = keras.backend\n",
        "embed_size = 128\n",
        "inputs = keras.layers.Input(shape=[None])\n",
        "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
        "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
        "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
        "z = keras.layers.GRU(128)(z, mask=mask)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "id": "4iCXHeEim0fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 훈련된 임베딩 재사용\n",
        "tf.random.set_seed(42)\n",
        "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
      ],
      "metadata": {
        "id": "a5piabwzml2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "model = keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
        "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "DYBPD7Ampzf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirpath, filename))"
      ],
      "metadata": {
        "id": "cQ2Ays6Xp07k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tensorflow_datasets\n",
        "\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_size = info.splits[\"train\"].num_examples\n",
        "batch_size = 32\n",
        "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
        "history = model.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "id": "QnlTKwFVqD1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 신경망 기계 번역을 위한 인코더-디코더 네트워크"
      ],
      "metadata": {
        "id": "lwhSK1WAq8tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "vocab_size = 100\n",
        "embed_size = 10"
      ],
      "metadata": {
        "id": "bgD7oQ7aqj7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "encoder_embeddings = embeddings(encoder_inputs)\n",
        "decoder_embeddings = embeddings(decoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(512, return_state=True) # LSTM층 만들 때 최종 은닉 상태를 디코더로 보냄\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "sampler = tfa.seq2seq.sampler.TrainingSampler() # 각 스텝에서 디코더에게 이전 스텝의 출력이 무엇인지 알려줌\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(512)\n",
        "output_layer = keras.layers.Dense(vocab_size)\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, \n",
        "                                                 output_layer=output_layer)\n",
        "final_outputs, final_state, final_sequence_lengths = decoder(\n",
        "    decoder_embeddings, initial_state=encoder_state, sequence_length=sequence_lengths)\n",
        "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
        "\n",
        "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
        "                           outputs=[Y_proba])"
      ],
      "metadata": {
        "id": "eKA3Xs5kr0fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "kEausHSFsNhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
        "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
        "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
        "seq_lengths = np.full([1000], 15)\n",
        "\n",
        "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
      ],
      "metadata": {
        "id": "EBeosx4fuahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 양방향 순환층\n",
        "# 하나는 앞에서 뒤로, 다른 하는 뒤에서 앞으로 읽음\n",
        "# 타임 스텝마다 두 출력을 연결\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
        "                                 keras.laeyrs.Bidirectional(keras.layers.GRU(10,return_sequences=True))\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SamEDWjoucKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빔검색\n",
        "# 모델의 앞선 실수를 고칠 수 있게 하는 것\n",
        "# k개의 가능성 있는 문장의 리스트를 유지하고 디코더 단계마다 이 문장의 단어를 하나씩 생성하여 가능성 있는 k개의 문장을 만듦\n",
        "# 파라미터 k를 빔 너비\n",
        "beam_width = 10\n",
        "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
        "    cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\n",
        "decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
        "    encoder_state, multiplier=beam_width)\n",
        "outputs, _, _ = decoder(embedding_decoder, start_token=start_tokens, end_token=end_token,\n",
        "                        initial_state=decoder_initial_state)"
      ],
      "metadata": {
        "id": "odYZDskqnB8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 어텐션 메커니즘"
      ],
      "metadata": {
        "id": "vMYWKMjetEn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 타임 스텝에서 적절한 단어에 디코더가 초점을 맞추도록 하는 기술\n",
        "attention_mechnism = tfa.seq2seq.attention_wrapper.LuongAttention(units, \n",
        "                                                                  encoder_state, memory_sequence_length=encoder_sequence_length)\n",
        "attention_decoder_cell = tfa.seq2seq2.attention_wrapper.AttentionWrapper(\n",
        "    decoder_cell, attention_mechanism, attention_later_size=n_units)"
      ],
      "metadata": {
        "id": "OhFGbJ4eqAT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머\n",
        "# 위치 인코딩\n",
        "class PositionalEncoding(keras.layers.Layer):\n",
        "  def __init__ (self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "    super().__init__(dtype=dtype, **kwargs)\n",
        "    if max_dims % 2 == 1: max_dims += 1 #max_dims 짝수\n",
        "    p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "    pos_emb = np.empty((1, max_steps, max_dims))\n",
        "    pos_emb[0, :, ::2] = np.sim(p / 10000**(2 * i / max_dims)).T\n",
        "    pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "    self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "def call(self, inputs):\n",
        "    shape = tf.shape(inputs)\n",
        "    return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
      ],
      "metadata": {
        "id": "s3wGYU1kyGZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 201\n",
        "max_dims = 512\n",
        "pos_emb = PositionalEncoding(max_steps, max_dims)\n",
        "PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()"
      ],
      "metadata": {
        "id": "PjxTP8o_1bMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qgb32RjE1oTE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}